# SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

services:
  api:
    build:
      context: ..
      dockerfile: infra/docker/api.Dockerfile
    restart: unless-stopped
    ports:
      - "8000:8000"
    env_file:
      - ../apps/api/.env
    environment:
      - DEBUG=false
      - STORAGE_BACKEND=${STORAGE_BACKEND:-local}
      - UPLOAD_DIR=/app/uploads
      - TEMPLATES_DIR=/app/templates
      - MAX_CONCURRENT_TRANSCRIPTIONS=1  # Reduzido de 3 para 1
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - NUMBA_DISABLE_CACHING=1
    volumes:
      - api_uploads:/app/uploads
      - api_logs:/app/logs
      - ../apps/api/.env:/app/.env:ro
    networks:
      - ambient-network
    deploy:
      resources:
        limits:
          memory: 1.5G  # Reduzido de 2G
        reservations:
          memory: 512M  # Reduzido de 1G
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import requests; requests.get('http://localhost:8000/api/health/', timeout=5)",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ui:
  #   build:
  #     context: ..
  #     dockerfile: infra/docker/ui.dev.Dockerfile
  #   restart: unless-stopped
  #   # No external ports - accessed through reverse proxy only
  #   env_file:
  #     - ../apps/ui/.env
  #   environment:
  #     - VITE_API_BASE=/api
  #   depends_on:
  #     - api
  #   networks:
  #     - ambient-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 512M
  #       reservations:
  #         memory: 256M
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "wget",
  #         "--no-verbose",
  #         "--tries=1",
  #         "--spider",
  #         "http://localhost:80/",
  #       ]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 3
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "5m"
  #       max-file: "3"

  # Nginx reverse proxy for production using approved Ubuntu 24.04 base
  # nginx:
  #   build:
  #     context: .
  #     dockerfile: docker/nginx.ubuntu.Dockerfile
  #   restart: unless-stopped
  #   ports:
  #     - "80:80" # HTTP traffic
  #     - "443:443" # HTTPS traffic
  #   volumes:
  #     - ./docker/nginx.conf:/etc/nginx/nginx.conf:ro
  #     - ./docker/default.conf:/etc/nginx/conf.d/default.conf:ro
  #     - ./nginx/ssl:/etc/nginx/ssl:ro
  #     - nginx_logs:/var/log/nginx
  #   depends_on:
  #     - ui
  #     - api
  #   networks:
  #     - ambient-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 256M
  #       reservations:
  #         memory: 128M
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "wget",
  #         "--no-verbose",
  #         "--tries=1",
  #         "--spider",
  #         "http://localhost:80/",
  #       ]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 3

  # NVIDIA Parakeet NIM for local Riva ASR
  # Use profile 'riva-nim' to deploy: docker-compose --profile riva-nim up
  # parakeet-nim:
  #   image: nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us:latest
  #   container_name: parakeet-1-1b-ctc-en-us
  #   restart: unless-stopped
  #   runtime: nvidia
  #   user: root
  #   environment:
  #     - NGC_API_KEY=${NGC_API_KEY}
  #     - NIM_HTTP_API_PORT=9000
  #     - NIM_GRPC_API_PORT=50051
  #     - NIM_TAGS_SELECTOR=name=parakeet-1-1b-ctc-en-us,mode=streaming,vad=silero,diarizer=sortformer
  #     - NIM_MODEL_PROFILE=5cdc247ecf3eabe3ab6d7e6018d507d6efb0ebc0c21c8e7b2cd9e23059175ffe
  #   ports:
  #     - "9000:9000"
  #     - "50051:50051"
  #   shm_size: 6gb  # Reduzido de 8gb para 6gb
  #   volumes:
  #     - parakeet-cache:/opt/nim/.cache
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 22G  # Aumentado de 20G para 22G (GPU T4 precisa de mais)
  #       reservations:
  #         memory: 12G  # Aumentado de 10G para 12G
  #         devices:
  #           - driver: nvidia
  #             device_ids: ["0"]
  #             capabilities: [gpu]
  #   networks:
  #     - ambient-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/v1/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 300s
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"

  # NVIDIA LLM NIM for self-hosted Llama-3.3-Nemotron-Super-49B
  # Use profile 'riva-nim' to deploy: docker-compose --profile riva-nim up
  # llama-nim:
  #   image: nvcr.io/nim/nvidia/llama-3.3-nemotron-super-49b-v1:latest
  #   container_name: llama-3.3-nemotron-super-49b-v1
  #   restart: unless-stopped
  #   # profiles:
  #   #   - riva-nim
  #   runtime: nvidia
  #   ports:
  #     - "8001:8000"
  #   shm_size: 16gb
  #   volumes:
  #     - nim-llm-cache:/opt/nim/.cache
  #   environment:
  #     - NIM_CACHE_PATH=/opt/nim/.cache
  #     - NGC_API_KEY=${NGC_API_KEY}
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=compute,utility
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ["all"]
  #             capabilities: [gpu]
  #   networks:
  #     - ambient-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
  #     interval: 60s
  #     timeout: 30s
  #     retries: 10
  #     start_period: 1800s
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"

  # NVIDIA Whisper NIM for local ASR
  whisper-nim:
    image: nvcr.io/nim/nvidia/whisper-large-v3:latest
    container_name: whisper-large-v3-turbo
    restart: unless-stopped
    runtime: nvidia
    user: root
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
      - NIM_HTTP_API_PORT=9000
      - NIM_GRPC_API_PORT=50051
      - NIM_TAGS_SELECTOR=name=whisper-large-v3
    ports:
      - "9000:9000"
      - "50051:50051"
    shm_size: 6gb
    volumes:
      - whisper-cache:/opt/nim/.cache
    deploy:
      resources:
        limits:
          memory: 18G  # Mais leve que Canary
        reservations:
          memory: 10G
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    networks:
      - ambient-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ambient-network:
    driver: bridge

volumes:
  api_uploads:
    driver: local
  api_logs:
    driver: local
  nim-llm-cache:
    driver: local
  parakeet-cache:
    driver: local
  whisper-cache:
    driver: local
